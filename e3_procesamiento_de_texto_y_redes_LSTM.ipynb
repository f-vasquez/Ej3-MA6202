{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-YiltMLCEeT"
   },
   "source": [
    "# MA6202: Laboratorio de Ciencia de Datos\n",
    "\n",
    "**Profesor: Nicolás Caro**\n",
    "\n",
    "**20/07/2020 - E3 S15**\n",
    "\n",
    "\n",
    "**Integrantes del grupo 6**: Danner Schlotterbeck, José Díaz V., Francisco Vásquez L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zejcg_ArCEeW"
   },
   "source": [
    "## Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "unIMQSF-CEeZ"
   },
   "source": [
    "El objetivo de esta evaluación es resolver un problema de detección de noticias falsas (*Fake News*) usando herramientas de aprendizaje de máquinas. \n",
    "\n",
    "Para lograr una representación numérica de los textos utilizaremos la librería `spaCy` para procesamiento de lenguaje natural. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFE7fvkSCEeb"
   },
   "source": [
    "**Instalaciones previas**  \n",
    "Para la ejecución correcta de este notebook puede ser necesario ejecutar los siguientes comandos de instalación:\n",
    "\n",
    "```python\n",
    "!pip install spacy tqdm\n",
    "!pip install -c pytorch torchtext\n",
    "!python -m spacy download en_core_web_sm\n",
    "```\n",
    "**Obs:** Puede usar conda en vez de pip si maneja su librería con esta herramienta. \n",
    "\n",
    "Las librerías que se instalan son:\n",
    "- spacy: ampliamente usada para procesamiento de lenguaje natural. Esta librería posee modelos estadísticos preentrenados como `en_core_web_sm` que será detallado posteriormente.\n",
    "- tqdm: para mostrar barras de progreso en pantalla.\n",
    "- torchtext: contiene en herramientas populares de procesamiento de lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DIRxIE22i5A"
   },
   "outputs": [],
   "source": [
    "!pip install spacy tqdm\n",
    "!pip install -c pytorch torchtext\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kixIAB7CEed"
   },
   "source": [
    "**Librerías**  \n",
    "En la evaluación, **no** estará permitido usar librerías ni módulos diferentes a los declarados en la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UfbYQ7bn2VFs"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mDw369JCEef"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report,\\\n",
    "    confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "'''\n",
    "Puede utilizar esta extension si trabaja en colaboratory:\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rA4Rg0XfCEeq"
   },
   "source": [
    "**Replicabilidad**  \n",
    "A lo largo de todo el ejercicio llamaremos múltiples veces a la función `np.random.seed`, con la semilla fija en la variable `seed_=300`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j5gBzo2SCEes"
   },
   "outputs": [],
   "source": [
    "seed_ = 300\n",
    "np.random.seed(seed_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ML4pIcNTjIy"
   },
   "source": [
    "**Uso de GPU**  \n",
    "En este ejercicio se utilizarán modelos que requieren alto poder de computo por lo que se recomienda usar GPU. Recuerde que en **Colaboratory** tiene acceso gratuito a dicho recurso.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OVVP3vtvUFSY"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wc0YidsCEez"
   },
   "source": [
    "## Preliminares\n",
    "\n",
    "### Carga de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QTqNQrWP34mq"
   },
   "source": [
    "- Compruebe que la siguiente celda coincide con este output:\n",
    "\n",
    "```\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 6335 entries, 8476 to 4330\n",
    "Data columns (total 3 columns):\n",
    " #   Column  Non-Null Count  Dtype \n",
    "---  ------  --------------  ----- \n",
    " 0   title   6335 non-null   object\n",
    " 1   text    6335 non-null   object\n",
    " 2   label   6335 non-null   object\n",
    "dtypes: object(3)\n",
    "memory usage: 198.0+ KB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FpKqWXBvCEe0"
   },
   "outputs": [],
   "source": [
    "raw_data_path = 'https://raw.githubusercontent.com/NicoCaro/DataScienceLab/master/ejercicios/ejercicio%203/data/news.csv'\n",
    "raw_df = pd.read_csv(raw_data_path, index_col=0)\n",
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHVO5t1KCEe7"
   },
   "source": [
    "**Preprocesamiento**  \n",
    "El conjunto de datos consta de 3 columnas:\n",
    "- `title`: contiene el título de la noticia\n",
    "- `text`: contiene el teto de la noticia\n",
    "- `label`: contiene las etiquetas `REAL` y `FAKE` que indican si se trata de una noticia verdadera o falsa.\n",
    "\n",
    "En la siguiente celda se incluye la columna `X` con una concatenación del título y el texto de las noticias, además de la columna `y` como una representación numérica de la columna `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XrFTxJiwCEe8"
   },
   "outputs": [],
   "source": [
    "pro_df = raw_df.copy()\n",
    "pro_df['y'] = (pro_df['label'] == 'FAKE').astype('int')\n",
    "pro_df['X'] = pro_df['title'].str.cat(pro_df['text'], sep='. ')\n",
    "\n",
    "# se eliminan las columnas innecesarias\n",
    "pro_df = pro_df.reindex(columns=['X', 'y'])\n",
    "display(pro_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dX2EcoxICEfC"
   },
   "source": [
    "### Procesamiento de texto\n",
    "A modo de ejemplo se muestra el procesamiento que se busca aplicar a cada una de las observaciones de la columna `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-RUH-Z0KCEfE"
   },
   "outputs": [],
   "source": [
    "x_muestra, y_muestra = pro_df.iloc[3].values.T\n",
    "print('Noticia falsa' if y_muestra else 'Noticia verdadera', '-' * 72,  sep='\\n')\n",
    "print(x_muestra[:501], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgLJ3uE1CEfJ"
   },
   "source": [
    "**Definición del modelo `spacy`**  \n",
    "El modelo estadístico de `spacy` que usaremos en el ejercicio es `english_web_sm`, que consiste en un objeto basado en una red convolucional, preentrenada en un conjunto de datos llamado Ontowords y diseñada para resolver múltiples tareas de procesamiento de lenguaje natural, dento de sus métodos se encuentran rutinas de tokenización y lematización detalladas posteriormente.\n",
    "\n",
    "Dentro de las funcionalidades que entrega este modelo, hay un subconjunto que no se utilizará en el ejercicio. Para ahorrar tiempo de cómputo, estas funcionalidades son deshabilitadas en el  el argumento `disable`.\n",
    "\n",
    "**Obs:** Puede ser necesario ejecutar `python -m spacy download en_core_web_sm` para tener acceso a tal modelo de lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhOaF6SDCEfK"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['entitry_ruler', 'textcat', \n",
    "                                            'entity_linker', 'ner', 'tagger'])\n",
    "print(type(nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUcaWEY4CEfP"
   },
   "source": [
    "**Tokenización**  \n",
    "\n",
    "Lo primero para analizar texto es separar el campo de texto en _tokens_. Un _token_ es un segmento significativo del texto. La entrada al tokenizer es un texto unicode, y la salida es un `spacy.tokens.doc.Doc`.\n",
    "\n",
    "\n",
    "El proceso puede entenderse como:\n",
    "1. Aplicar el método `str.split(' ')` que entrega una lista de `str`.\n",
    "2. Verificar si cada uno de los elementos de la lista puede subdividirse:\n",
    "    1. **Porque se trata de una regla de excepción.** Por ejemplo `don't` debería subdividirse en `do` y `n't`, mientras que `U.K.` no debe subdividirse.\n",
    "    2. **Porque el elemento contiene prefijos, sufijos o [infijos](https://dle.rae.es/infijo).** Por ejemplo comillas, comas, puntos, etc...\n",
    "    \n",
    "Para más detalles ver la documentación de [spacy](https://spacy.io/usage/linguistic-features#tokenization).\n",
    "\n",
    "Así al generar *tokens* en `x_muestra`, los 20 primeros son: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRloFA6CCEfP"
   },
   "outputs": [],
   "source": [
    "spacy_doc = nlp(x_muestra)\n",
    "[print(token) for token in spacy_doc[:20]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oB5xDaiXCEfW"
   },
   "source": [
    "**Lematización**  \n",
    "La [lematización](https://es.wikipedia.org/wiki/Lematizaci%C3%B3n) es el proceso de agrupar las formas flexionadas de una palabra (en plural, en femenino, conjugada, etc), para que puedan analizarse como un solo elemento, identificado por el **lema** de la palabra.\n",
    "\n",
    "En procesamiento de lenguaje natural a lematización depende de la identificación correcta de la [categoría gramatical](https://es.wikipedia.org/wiki/Categor%C3%ADa_gramatical) (*part of speech*). Algunos ejemplos de categoría gramatical son sustantivo, adjetivo, verbo, advervio, etc...\n",
    "\n",
    "En `spacy` accedemos al lema mediante el atributo `lemma_`. Así, para los 20 primeros *tokens*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKskjZS8CEfW"
   },
   "outputs": [],
   "source": [
    "[print(token.lemma_) for token in spacy_doc[:20]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0jrekzgCEfb"
   },
   "source": [
    "**Stopwords**  \n",
    "Los *tokens* generados pueden ser clasificados como [palabras vacías](https://es.wikipedia.org/wiki/Palabra_vac%C3%ADa) (*stop words*) que no tienen significado en si mismas. Algunos ejemplos son preposiciones, artículos, pronombres, etc... En procesamiento de lenguaje natural es común eliminarlas.\n",
    "\n",
    "Para este ejercicio eliminaremos las *palabras vacías* mediante el atributo `is_stop`, además de los *token* que no sean alfanuméricos mediante el atributo `is_alpha`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HuBvA32OCEfc"
   },
   "outputs": [],
   "source": [
    "[print(token) for token in spacy_doc[:20] if (not token.is_stop) and token.is_alpha];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3x2F0cVCEfi"
   },
   "source": [
    "#### Pregunta 1\n",
    "1. Defina la función `procesa_texto` que reciba como argumento un texto y un modelo de lenguaje `spacy`. La función `procesa_texto` debe entregar un texto con los lemas de cada *token*, eliminando las palabras vacías y los *token* que no sean alfanuméricos, usando el procedimiento recién presentado. El texto obtenido debe separar los lemas por espacios `' '`.\n",
    "2. Aplique dicha función a la columna `X`, guardando sus resultados en la columna `pro_X`. Como modelo de lenguaje `spacy`, utilice el modelo instanciado anteriormente.  \n",
    "    **Observaciones**:\n",
    "    - Es usual que esta operación tome bastante tiempo por lo que recomendamos usar el método `progress_apply` de pandas que permite reportar el progreso de la operación mediante la librería `tqdm`. Para habilitar dicho método de la librería pandas debe primero llamar al método `tqdm.pandas`.\n",
    "    - Puede ser útil guardar el resultado de esta operación en disco, dado el tiempo que toma repetir la operación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbBeTLWShG86"
   },
   "outputs": [],
   "source": [
    "def procesa_texto(in_text, spacy_model):\n",
    "\n",
    "    out_text = ''\n",
    "    spacy_doc = spacy_model(in_text)\n",
    "\n",
    "    for token in spacy_doc:\n",
    "    if (not token.is_stop) and token.is_alpha:\n",
    "        out_text += \" \" + token.text\n",
    "    return out_text.lstrip()\n",
    "\n",
    "tqdm.pandas()\n",
    "pro_df['pro_X'] = pro_df['X'].progress_apply(procesa_texto, args=(nlp,))\n",
    "\n",
    "pro_df.to_pickle('pro_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_QePI5bCEfq"
   },
   "source": [
    "3. Cuente el número de lemas en cada observación de la columna `pro_X` y compruebe que obtiene las siguientes estadísticas descriptivas:\n",
    "\n",
    "```\n",
    "count    6335.000000\n",
    "mean      393.084294\n",
    "std       409.950812\n",
    "min         2.000000\n",
    "25%       153.000000\n",
    "50%       311.000000\n",
    "75%       515.000000\n",
    "max      8730.000000\n",
    "Name: pro_X, dtype: float64\n",
    "```\n",
    "*Hint*: puede ser útil el método `pd.Series.str.count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWGdXsbPD4ow"
   },
   "outputs": [],
   "source": [
    "pro_df = pd.read_pickle('pro_df.pkl')\n",
    "\n",
    "num_lem = pro_df['pro_X'].str.count(' ') + 1\n",
    "num_lem.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_nsPKTtCEfv"
   },
   "source": [
    "4. Instancie `final_df` como una copia de `pro_df` a la que se le elimina la columna `'X'`. Luego aplique la eliminación de los duplicados `final_df`.   \n",
    "- Compruebe que obtiene un total de 6,303 filas únicas en `final_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_88ZW5V4D-7c"
   },
   "outputs": [],
   "source": [
    "final_df = pro_df.loc[:, pro_df.columns != 'X'].copy()\n",
    "\n",
    "final_df.drop_duplicates(inplace=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbojUolFCEf2"
   },
   "source": [
    "### Definición de notación\n",
    "De ahora en adelante denotamos por:\n",
    "- ***Documento***: cada una de las noticias procesadas del conjunto de datos, es decir, cada una de las observaciones de la columna `'pro_X'`del DataFrame `final_df`.\n",
    "- ***Corpus***: el conjunto de *documentos* del conjunto de datos, es decir, el conjunto de observaciones de la columna `'pro_X'` del DataFrame `final_df`.\n",
    "- ***Vocabulario***: al conjunto de *tokens* presentes en el *corpus*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySZAaNLlCEf3"
   },
   "source": [
    "### Definición de conjuntos del problema\n",
    "Se separan las muestras en dos conjuntos:\n",
    "- Conjunto de *entrenamiento union validación* (denotado por `*_full_train`), con el 80% de las observaciones\n",
    "- Conjunto de *prueba* (denotado por `*_test`), con el 20% de las observaciones.\n",
    "\n",
    "A su vez el conjunto de *entrenamiento union validación* de subdivide en:\n",
    "- Conjunto de *entrenamiento* (denotado por `*_train`), con el 64% de las observaciones.\n",
    "- Conjunto de *validación* (denotado por `*_val`), con el 16% de las observaciones\n",
    "\n",
    "Para realizar esta subdivisión utilizamos dos veces la función `sklearn.model_selection.train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2YYSQ9p4CEf7"
   },
   "outputs": [],
   "source": [
    "# define parametros de la division de conjuntos\n",
    "proporcion_total_entrenamiento_prueba = 0.80\n",
    "proporcion_entrenamiento_validacion = 0.80\n",
    "\n",
    "# obtiene conjunto de prueba\n",
    "X_full_train, X_test, y_full_train, y_test= train_test_split(\n",
    "    final_df.pro_X, final_df.y, train_size=proporcion_total_entrenamiento_prueba, \n",
    "    random_state=seed_)\n",
    "\n",
    "# obtiene conjunto de entrnamiento y validacion\n",
    "X_train, X_val, y_train, y_val  = train_test_split(\n",
    "    X_full_train, y_full_train, train_size=proporcion_entrenamiento_validacion, \n",
    "    random_state=seed_)\n",
    "\n",
    "# guardar los conjuntos en formato csv\n",
    "conjuntos_dir = 'conjuntos'\n",
    "os.makedirs(conjuntos_dir, exist_ok=True)\n",
    "guarda_csv = lambda X, y, filename: pd.DataFrame({\n",
    "    'pro_X': X, 'y': y\n",
    "}).to_csv(f'{conjuntos_dir}/{filename}.csv', index=None)\n",
    "guarda_csv(X_train, y_train, 'entrenamiento')\n",
    "guarda_csv(X_val, y_val, 'validacion')\n",
    "guarda_csv(X_test, y_test, 'prueba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDRxP4ebCEf_"
   },
   "source": [
    "## Modelos de aprendizaje de máquinas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAg8An6BEsDg"
   },
   "source": [
    "### Modelos no paramétricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kljsRVFCEgA"
   },
   "source": [
    "#### Representación por conteo de ocurrencias.\n",
    "Dado que se busca resolver un problema de clasificación de documentos, es necesario representar los documentos de forma numérica. A continuación usaremos la representación por conteo de apariciones de cada uno de los *tokens* presentes en el *vocabulario*.\n",
    "\n",
    "Para realizar esta vectorización de documentos se utiliza la clase `sklearn.feature_extraction.text.CountVectorizer`. El método `fit_transform` de esta clase  recibe como argumento un `iterable` de *documentos*, extrae el *vocabulario* de dicho `iterable` y retorna la matriz de número de ocurrencias de cada *token* del *vocabulario*, en cada uno de los documentos del `iterable`. En otras palabras, definiendo $\\text{tf}(t_i, \\mathbf{d}_j)$ como el número de apariciones del *token* $t_i$ en el *documento* $\\mathbf{d}_j$, `~CountVectorizer.fit_transform` retorna la matriz $\\mathbf{C}$ definida por:\n",
    "$$\\big(c_{i, j}\\big) = \\text{tf}(t_j, \\mathbf{d}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14YOBySYCEgB"
   },
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "Se emplea el algoritmo de *Naive Bayes* como base de referencia para los modelos más complejos empleados posteriormente. Específicamente, se utiliza una instancia de la clase `sklearn.naive_bayes.MultinomialNB` que está diseñado para trabajar con las características del tipo conteo. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LXS7kANE0A-"
   },
   "source": [
    "##### Pregunta 2\n",
    "1. Instancie `nb_pipe` como un objeto de la clase `sklearn.pipeline.Pipeline` con los componentes:\n",
    "    - `~CountVectorizer` inicializado con `max_features=20000` los demás parámetros por defecto.\n",
    "    - `~MultinomialNB` inicializado con los parámetros por defecto.  \n",
    "    \n",
    "   Posteriormente, ajuste `nb_pipe` en el conjunto de *entrenamiento union validación* y guarde el modelo resultante en la carpeta `modelos/nb_pipe.pk` como un archivo `pickle`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBiY5mcwEC1N"
   },
   "outputs": [],
   "source": [
    "nb_pipe = Pipeline([('CountVectorizer', CountVectorizer(max_features=20000)),\n",
    "                                      ('MultinomialNB', MultinomialNB())])\n",
    "\n",
    "nb_pipe.fit(X_full_train, y_full_train)\n",
    "\n",
    "path = '/modelos/'\n",
    "#path = '/content/drive/My Drive/U_Proyectos/DataScienceLab/Ejercicio3/modelos/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "pipe_file = open(path + 'nb_pipe.pk', 'wb')\n",
    "pk.dump(nb_pipe, pipe_file)\n",
    "pipe_file = open(path + 'nb_pipe.pk', 'rb')\n",
    "nb_pipe = pk.load(pipe_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8R4IVhILCEgG"
   },
   "source": [
    "2. Defina la función `evalua_sklearn`  que reciba como argumentos:\n",
    "    - `y_true`: np.array de una dimensión, conteniendo las etiquetas de cada una de las observaciones \n",
    "    - `y_pred`: np.array, con las etiquetas predichas por algún modelo de clasificación\n",
    "    - `nombre_clasificador`: str, define el nombre de la carpeta donde los resultados son guardados. \n",
    "    \n",
    "  Esta función debe:\n",
    "    - Imprimir en pantalla los resultados de clasificación, mediante `sklearn.metrics.classification_report` con 4 dígitos de precisión. Además debe guardar dichos resultadoes  en la ruta `f'resultados/{nombre_clasificador}/reporte_clasificacion.txt'`.\n",
    "    - Generar un gráfico con la *matriz de confusión* mediante el uso de `sklearn.metrics.confusion_matrix` y `seaborn.heatmap`. Además debe guardar dicho gráfico en la ruta `f'resultados/{nombre_clasificador}/mc.pdf'`.\n",
    "    \n",
    "  Pruebe esta función con la predicción de `nb_pipe` sobre el conjunto de *prueba*, usando `nombre_clasificador='nb_pipe'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opHsYtdSEH6J"
   },
   "outputs": [],
   "source": [
    "def evalua_sklearn(y_true, y_pred, nombre_clasificador):\n",
    "    #path = 'resultados/' + nombre_clasificador + '/'\n",
    "    path = 'resultados/' + nombre_clasificador + '/'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred, digits=4)\n",
    "    print(class_report)\n",
    "    report_file = open(path + 'reporte_clasificacion.txt', 'w')\n",
    "    report_file.write(class_report)\n",
    "    report_file.close()\n",
    "\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred, normalize='all')\n",
    "    hm = sns.heatmap(cm, annot=True)\n",
    "    hm.get_figure().savefig(path + 'mc.pdf')\n",
    "    plt.show()\n",
    "\n",
    "#print(nb_pipe.score(X_test, y_test))\n",
    "y_pred = nb_pipe.predict(X_test)\n",
    "evalua_sklearn(y_true=y_test, y_pred=y_pred, nombre_clasificador='nb_pipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQ1wjJUqCEgM"
   },
   "source": [
    "- Compruebe que obtiene un *accuracy* y un promedio ponderado de *f1-score* superiores a .89.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F29MRA-xCEgN"
   },
   "source": [
    "#### Representación Tf-idf\n",
    "\n",
    "Para esta sección y la posterior se emplea la clase `sklearn.feature_extraction.text.TfidfVectorizer`. \n",
    "\n",
    "Para comprender el algoritmo ***tf-idf*** (*term frequency - inverse document frequency*) necesitamos definir 3 de sus componentes:\n",
    "- $\\text{tf}(t_j, \\mathbf{d}_i)$ que representa el número de apariciones del *token* $t_j$ en el *documento* $\\mathbf{d}_i$.\n",
    "\n",
    "- $\\text{df}(t_j)$ que representa el número de documentos en que aparece el *token* $t_j$.\n",
    "- $\\text{idf}(t_j)$ que representa el inverso de la frecuencia del *token* $t_j$ en los *documentos* del *corpus*. En el caso particular del objeto `~.TfidfVectorizer` empleado, denotando por $n$ al numero de documentos del *corpus*, se usa una versión suavizada de la función $\\text{idf}$. Esto es: \n",
    "\n",
    "$$\n",
    "\\text{idf}(t_j) = \\log(\\frac{1 + n}{1 + \\text{df}(t_j)}) + 1\n",
    "$$\n",
    "\n",
    "\n",
    "Con dichos componentes, podemos definir a la representación vectorial del documento $\\mathbf{d}_i$ mediante el vector $\\textbf{tf-idf}(\\mathbf{d}_i)$, cuyas coordenadas se calculan de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\left(\\textbf{tf-idf}(\\mathbf{d}_i)_i\\right) = \\text{tf}(t_j, \\mathbf{d}_i) \\times \\text{idf}(t_j)\n",
    "$$\n",
    "\n",
    "\n",
    "Así la salida del objeto `~.TfidfVectorizer` es la matriz de la concatenación por filas, de los vectores $\\textbf{tf-idf}(\\mathbf{d}_i)$ normalizados según la norma euclidiana, es decir, la matriz $\\mathbf{X}$, definida por:\n",
    "\n",
    "\n",
    "$$\n",
    "\\left( x_{i,j} \\right) = \\frac{\\text{tf}(t_j, \\mathbf{d}_i) \\times \\text{idf}(t_j)}{{||\\textbf{tf-idf}(\\mathbf{d}_i)||}_2} \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "donde ${||\\cdot||}_2$ representa la norma euclidiana. En consecuencia se logra una representación vectorial sobre la esfera unitaria de la norma ${||\\cdot||}_2$.\n",
    "\n",
    "Una interpretación posible de esta representación vectorial es que cada *token* $t_j$ tiene mayor importancia en el documento $\\mathbf{d}_i$:\n",
    "\n",
    "- En la medida en que esta aparezca más veces en el documento\n",
    "- En la medida en que esta aparezca menos veces en los demás documentos del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M8A8IZPoCEgN"
   },
   "source": [
    "#### Maquinas de soporte vectorial (SVM)\n",
    "\n",
    "Se emplea el algoritmo de máquinas de soporte vectorial (*Support Vector Machines - SVM*) sobre la representación vectorial *tf-idf*. La clase utilizada para generar este modelo es `sklearn.svm.SVC`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_rQh6ZPE4dA"
   },
   "source": [
    "##### Pregunta 3\n",
    "1. Instancie `svm_pipe` como un objeto de la clase `sklearn.pipeline.Pipeline` con los componentes:\n",
    "    - `~.TfidfVectorizer` inicializado con `max_features=20000` los demás parámetros por defecto.\n",
    "    - `~.SVC` inicializado con el kernel RBF y los parámetros por defecto.  \n",
    "    \n",
    "   Posteriormente, ajuste `svm_pipe` en el conjunto de *entrenamiento union validación* y guarde el modelo resultante en la carpeta `modelos/svm_pipe.pk` como un archivo `pickle`. Finalmente, reporte el desempeño del clasificador mediante `evalua_sklearn` con la predicción de `svm_pipe` sobre el conjunto de *prueba*, usando `nombre_clasificador='svm_pipe'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-fRguPeELN-"
   },
   "outputs": [],
   "source": [
    "svm_pipe = Pipeline([('TfidfVectorizer', TfidfVectorizer(max_features=20000)),\n",
    "                                      ('SVM', SVC(kernel='rbf'))])\n",
    "\n",
    "svm_pipe.fit(X_full_train, y_full_train)\n",
    "\n",
    "path = '/modelos/'\n",
    "#path = '/content/drive/My Drive/U_Proyectos/DataScienceLab/Ejercicio3/modelos/'\n",
    "pipe_file = open(path + 'svm_pipe.pk', 'wb')\n",
    "pk.dump(svm_pipe, pipe_file)\n",
    "pipe_file = open(path + 'svm_pipe.pk', 'rb')\n",
    "svm_pipe = pk.load(pipe_file)\n",
    "\n",
    "y_pred = svm_pipe.predict(X_test)\n",
    "evalua_sklearn(y_true=y_test, y_pred=y_pred, nombre_clasificador='svm_pipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xj3HbM47CEgT"
   },
   "source": [
    "Las máquinas de soporte vectorial son algoritmos que pueden ser muy sensibles a los hiperparámetros. Por esta razón es útil emplear un esquema de validación cruzada. A continuación se implementa un esquema de validación cruzada simple, que explora sólo diferentes kernels y coeficientes de regularización.\n",
    "2. Instancie `svm_grid` como un objeto de la clase `sklearn.model_selection.GridSearchCV`con los parámetros:\n",
    "    - `n_jobs=-1` para usar todos los núcleos disponibles\n",
    "    - `param_grid` definido de tal forma que le permita probar las combinaciones de los siguientes hiperparámetros de `~SVC`:\n",
    "        - `kernel` en {`'lineal'`, `'rbf'`}\n",
    "        - `C` en {`.01`, `.1`, `1`, `10`, `100`}\n",
    "\n",
    "    - `cv=3` para generar un esquema de validación cruzada estratificada con 3 *fold*.\n",
    "    - `verbose=1` para reportar el progreso del ajuste en pantalla\n",
    "    \n",
    "  Los demás parámetros quedan con sus valores por defecto.  \n",
    "  \n",
    "  Ajuste `grid_search_svm` usando `svm_pipe` sobre el conjunto *entrenamiento union validación* y guarde el modelo resultante en la carpeta `modelos/svm_grid.pk` como un archivo `pickle`. Luego reporte el desempeño del clasificador mediante `evalua_sklearn` con la predicción de `svm_grid` sobre el conjunto de *prueba*, usando `nombre_clasificador='svm_grid'`.  \n",
    "  \n",
    "  **Obs**: Evite fuga de información al combinar ``svm_pipe`` con `grid_search_svm`.\n",
    "\n",
    "  Finalmente, en ruta `resultados/svm_grid/mejores_parametros.txt` guarde los mejores parámetros obtenidos en `svm_grid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJJyKChuEOLr"
   },
   "outputs": [],
   "source": [
    "# no olvide fijar la semilla \n",
    "np.random.seed(seed_)\n",
    "\n",
    "svm_pipe = Pipeline([('TfidfVectorizer', TfidfVectorizer(max_features=20000)),\n",
    "                                      ('SVM', SVC())])\n",
    "parameters = {'SVM__kernel':('lineal', 'rbf'), 'SVM__C':[.01, .1, 1, 10, 100]}\n",
    "svm_grid = GridSearchCV(estimator = svm_pipe, param_grid=parameters, n_jobs=-1, cv=3, verbose=1)\n",
    "\n",
    "svm_grid.fit(X_full_train, y_full_train)\n",
    "\n",
    "y_pred = svm_grid.predict(X_test)\n",
    "evalua_sklearn(y_true=y_test, y_pred=y_pred, nombre_clasificador='svm_grid')\n",
    "\n",
    "print(svm_grid.best_params_)\n",
    "path = 'resultados/svm_grid/'\n",
    "# path = '/content/drive/My Drive/U_Proyectos/DataScienceLab/Ejercicio3/resultados/svm_grid/'\n",
    "with open(path + 'mejores_parametros.txt', 'w') as file:\n",
    "    file.write(str(svm_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CW8MfC6GEdgD"
   },
   "source": [
    "### Modelos paramétricos\n",
    "\n",
    "En lo que sigue del ejercicio se construirá una red neuronal recurrente (*RNN*). Para ello se usará principalmente la librería `torchtext` que provee una serie de herramientas que facilitan el manejo de texto para redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqXRozFCFCYS"
   },
   "source": [
    "#### Representación vectorial densa\n",
    "\n",
    "En las representaciones vectoriales anteriores, cada *token* del *vocabulario* constituye una dimension del espacio, es decir, hay tantas dimensiones en el espacio vectorial de representación como *tokens* en el *vocabulario*.\n",
    "\n",
    "Hay una desventaja enorme en esta representación, además de la cantidad enorme de dimensiones que genera. Esta es que básicamente trata todos los *token* como entidades independientes sin relación entre sí. Lo que se busca en una representación densa es alguna noción de similitud entre las palabras.\n",
    "\n",
    "Por ejemplo, supoga que se construye un modelo de lenguaje y que hemos visto las oraciones:\n",
    "\n",
    "- El matemático corrió a la tienda.\n",
    "- El físico corrió a la tienda.\n",
    "- El matemático resolvió un problema abierto.\n",
    "\n",
    "en el conjunto de *entrenamiento*. Ahora supongamos que obtenemos una nueva oración no presente en el conjunto de *entrenamiento*:\n",
    "\n",
    "- El físico resolvió un problema abierto.\n",
    "\n",
    "Si bien el modelo de lenguaje puede funcionar bien en esta oración, sería mejor si se pudiera utilizar los siguientes aspectos:\n",
    "\n",
    "- Se ha observado matemático y físico en el mismo papel en una oración. De alguna manera tienen una relación semántica.\n",
    "- Se ha observado al matemático en el rol del físico en una oración análoga a esta nueva oración.\n",
    "\n",
    "Así, se podría inferir que el físico en realidad encaja bien en la nueva oración. Esto esconde una noción de similitud: queremos decir similitud semántica, no simplemente tener representaciones ortográficas similares. Este ejemplo, por supuesto, se basa en una suposición lingüística fundamental: que las palabras que aparecen en contextos similares están relacionadas semánticamente entre sí. Esto se llama hipótesis ***distribucional***.\n",
    "\n",
    "Sobre dicha hipotesis se basa la construcción de modelos de procesamiento de lenguaje natural llamado ***word embeddings*** que se utilizan a continuación.\n",
    "\n",
    "<center>Explicación adaptada de <a href=\"https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\">Word Embedding: Encoding Lexical Semantics - Pytorch</a></center> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRUh3opLLJ3P"
   },
   "source": [
    "##### Pregunta 4 \n",
    "La forma de cargar datos en formato csv que provee `torchtext`, es mediante los objetos `torchtext.data.dataset.TabularDataset`. A direfencia de `pandas.read_csv`, donde la función puede inferir el tipo de dato de cada columna, `~.TabularDataset` necesita que dichos tipos de dato sean declarados, mediante el objeto `torchtext.data.Field`.\n",
    "\n",
    "1. Instancie `train_td`, `val_td` y `test_td` mediante el método `~.TabularDataset.splits` que le permita cargar textos de los archivos `conjuntos/*.csv` respectivos. Para ello tendrá que instanciar dos objetos de la clase `~.Field` que definan el tipo de dato para cada columna:\n",
    "\n",
    "- La columna `y`: declarada mediante el objeto `etiqueta_fd` como instancia de `~.Field`, con parametros `sequential=False, use_vocab=False, batch_first=True, dtype=torch.float` y los demás por defecto.\n",
    "- La columna `pro_X`:  declarada mediante el objeto `documento_fd`, como instancia de `~.Field`, con parámetros `include_lengths=True, batch_first=True` y los demás por defecto. El primer parámetro (`include_lengths=True`) implica que cada *documento* cargado de la columna se entregará en forma de tupla, donde además de la secuencia de *tokens* de aquel *documento*, se adjuntará el largo de la secuencia o número de tokens contenidos en este.\n",
    "\n",
    "**Obs**: Tendrá que inferir los parámetros con los cuales emplear el método `~.splits()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hGI0K4BRz33u"
   },
   "outputs": [],
   "source": [
    "documento_fd = Field(include_lengths = True,\n",
    "                     batch_first = True)\n",
    "etiqueta_fd = Field(sequential = False,\n",
    "                    use_vocab = False,\n",
    "                    batch_first = True,\n",
    "                    dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnDXhEds2UyJ"
   },
   "outputs": [],
   "source": [
    "train_td, val_td, test_td = TabularDataset.splits(\n",
    "    path = conjuntos_dir+'/',\n",
    "    train = 'entrenamiento.csv',\n",
    "    validation = 'validacion.csv',\n",
    "    test = 'prueba.csv',\n",
    "    format = 'csv',\n",
    "    fields = [('pro_X', documento_fd),\n",
    "              ('y', etiqueta_fd)],\n",
    "              skip_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZQsjBrESIYZ"
   },
   "source": [
    "`torchtext` también provee objetos que permiten iterar sobre un objeto de tipo `~.TabularDataset`. En particular, en el procesamiento de texto es beneficioso generar *batches* de secuencias que tengan largo similar, para ahorrar tiempo de escritura, ya que dichos *batches* al tener forma tensorial, deben ser completados por ceros (*padding*). Así por ejemplo, las secuencias:\n",
    "```\n",
    "[ [4, 16, 3, 8],\n",
    "  [5, 2],\n",
    "  [6, 6, 7, 9, 2] ]\n",
    "```\n",
    "deben ser transformadas a:\n",
    "```\n",
    "[ [4, 16, 3, 8, 0],\n",
    "  [5, 2, 0, 0, 0],\n",
    "  [6, 6, 7, 8, 2] ]\n",
    "```\n",
    "\n",
    "Mediante la clase ` torchtext.data.BucketIterator`, es posible iterar sobre instancias de `~.TabularDataset`, de manera que se minimiza la cantidad de *padding* y al mismo tiempo se mantiene un orden aleatorio de los datos.\n",
    "\n",
    "2.  Instancie *iterators* de cada uno de los `~.TabularDataset` instanciados, mediante `~.BucketIterator.splits` con los parámetros `batch_size=32, sort_key=lambda x: len(x.pro_X),\n",
    "device=device, sort=True, sort_within_batch=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvPAvN1eUX9E"
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "    (train_td, val_td, test_td), batch_size = 32,\n",
    "    sort_key=lambda x: len(x.pro_X), device = device,\n",
    "    sort = True, sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PI4uDB6mUWd2"
   },
   "source": [
    "3. Obtenga la representación vectorial del vocabulario con el *word embedding* GloVe entrenado en [Wikipedia 2014 + GigaWord 5](https://nlp.stanford.edu/projects/glove/). Para ello:\n",
    "- Instancie el objeto `glove` como una instancia de la clase `torchtext.vocab.GloVe` con los parámetros`name=\"6B\", dim=300`.\n",
    "- Ejecute el método `build_vocab` del objeto `documento_fd` con `max_size=20000` y los demás parámetros que correspondan   \n",
    "  Compruebe posteiormente que al llamar a `documento_fd.vocab.vectors.shape` obtiene las dimensiones `(20002,300)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5l65cImC6in6"
   },
   "outputs": [],
   "source": [
    "glove = GloVe(name = '6B',\n",
    "              dim = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLY_wY_V7ZyX"
   },
   "outputs": [],
   "source": [
    "documento_fd.build_vocab(train_td, vectors = glove, max_size = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PvLwRjIX309K"
   },
   "outputs": [],
   "source": [
    "documento_fd.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQ875UiZYrY4"
   },
   "source": [
    "#### LSTM \n",
    "Finalmete se implementa una LSTM bidireccional. La estructura de la red LSTM ya fue revisada en el material del curso, sin embargo, no se estudió su variante bidireccional. Esta denominación implica que la secuencia de tokens presente en cada texto es procesada desde el primer token hasta el último en una LSTM y desde el úĺtimo hasta el primero en otra LSTM, tal como lo ilustra la siguiente imagen.\n",
    "\n",
    "<center> <img src=\"http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-bidirectional.png\" align=\"middle\"> </center>\n",
    "<center> Fuente: <a href=\"http://colah.github.io/posts/2015-09-NN-Types-FP/\"> Colah's Blog </a></center>\n",
    "\n",
    "Por consistencia con la imagen, supongamos que la LSTM bidireccional recibe un documento compuesto por la secuencia de tokens $x_0, \\ldots, x_u$, de largo $i+1$. Sean $\\text{LSTM}$ la RNN que recibe los inputs desde $0$ hasta $i$ y $\\text{LSTM}'$ la RNN que los recibe desde $i$ hasta $0$. A cada input de la secuencia, $x_j$, corresponde un *output*, $y_j$ que consiste en la concatenación $\\left(A(S_j, x_j),A'(S'_{i-j}, x_j)\\right)$. Lo que debe ser considerado como la salida final de la LSTM bidireccional es la primera mitad de $y_i$ y la segunda mitad de $y_0$ pues corresponden a $A(S_i, x_i)$ y $A'(S'_i, x_0)$ respectivamente. Observe que este es output se asocia a la secuencia completa.\n",
    "\n",
    "La LSTM bi-direccional que se implementa está diseñada para usar un *word embedding* fijo, como el que fue calculado en la sección anterior.\n",
    "\n",
    "La estructura de la red es la siguiente:\n",
    "- *Word embedding* pre-entrenado (no deben calcularse gradientes en esta sección de la red), implementado con la clase `torch.nn.Embedding`.\n",
    "- LSTM bidireccional, tomando como salida la concatenación recién explicada e implementada con la clase `torch.nn.LSTM` con los parámetros `batch_first=True` y `bidirectional=True`. \n",
    "- Dropout con probabilidad .5\n",
    "- Capa totalmente conectada con salida de tamaño 1 y función de activación sigmoide.\n",
    "\n",
    "Dado que `~.BucketIterator` entrega un tensor con *paddings* de cero, es necesario transformar el input de `~.LSTM`, de tal forma que esta no procese los ceros del tensor de entrada. Por esta razón al pasar del output de `~.Embedding`, al input de `~.LSTM`, es necesario emplear la función `torch.nn.utils.rnn.pack_padded_sequence` que permite transformar una secuencia con *paddings* de 0, en una secuencia que los oculta. Dicha función es capaz de transformar un input de la forma:\n",
    "```\n",
    "seq = torch.tensor([[4,5,6], [1,2,0], [3,0,0]])\n",
    "lens = [3, 2, 1]\n",
    "packed = pack_padded_sequence(seq, lens, batch_first=True, enforce_sorted=True)\n",
    "```\n",
    "donde el objeto `packed` tendrá la forma:\n",
    "```\n",
    "PackedSequence(data=tensor([4, 1, 3, 5, 2, 6]), batch_sizes=tensor([3, 2, 1]), sorted_indices=None, unsorted_indices=None)\n",
    "```\n",
    "y estará listo para ser procesado como entrada de `~.LSTM`.\n",
    "\n",
    "Sin embargo, el empleo de aquella función, obliga a usar su función inversa, `torch.nn.utils.rnn.pad_packed_sequence`, sobre la salida de `~.LSTM` y así que pasar de un objeto `PackedSequence` a su formato tensorial con *padding* de 0. En consecuencia al emplear:\n",
    "```\n",
    "lstm = nn.LSTM(...)\n",
    "packed_output, _ = lstm(packed)\n",
    "seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=True)\n",
    "``` \n",
    "Esto permitirá encontrar el output de la LSTM bidireccional en el tensor `seq_unpacked` de tamaño $B \\times T \\times C$, donde $B$ representa el tamaño del *batch*, $T$ el largo de la secuencia más larga del *batch* y $C$ el tamaño de la dimension del espacio de carácteristicas de la salida de `~.LSTM`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0r7-VImYqgc"
   },
   "source": [
    "##### Pregunta 5\n",
    "\n",
    "1. Defina la clase `Glove6B300BiLSTM` heredando de `torch.nn.Module` y definiendo los métodos:\n",
    "- `__init__`: recibe como parametros `hidden_size`, que regula el parámetro homónimo de `torch.nn.LSTM`; y `text_field` que apunta al `torchtext.data.Field` que ya contiene la representación vectorial densa del *vocabulario* que es utilizado en la capa del *word embedding*.  \n",
    "- `forward`: recibe como argumentos `text` y `text_len` que corresponden a los objetos que entrega `BucketIterator`.  \n",
    "Haga uso de las funciones `~.pack_padded_sequence` con parámetros `batch_first=True, enforce_sorted=True` y `~.pad_packed_sequence`, con parámetro `batch_first=True`. Sea ciudadoso en la selección de los segmentos del tensor de salida de `~.pad_packed_sequence` que deben ser considerados para las capas posteriores.   \n",
    "\n",
    "*Hint*: note que para construir el output de la $\\text{LSTM}$ bidireccional, deberá seleccionar segmentos de un vector de salida y concatenarlos de manera conveniente. Para ello le será de ayuda la variable `text_len`.\n",
    "\n",
    "  \n",
    "Instancie `modelo` como objeto de la clase `Glove6B300BiLSTM` con `hidden_size=128` y `text_field=documento_fd`. Recuerde instanciarlo en el espacio de memoria adecuado mediante el método `to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iXgixLg3f_Pc"
   },
   "outputs": [],
   "source": [
    "# Definir la red\n",
    "class Glove6B300BiLSTM(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size, \n",
    "                 text_field):\n",
    "        super().__init__()\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            text_field.vocab.vectors.clone(),\n",
    "            freeze = True\n",
    "        )\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(text_field.vocab.vectors.shape[1],\n",
    "                           hidden_size,\n",
    "                           bidirectional = True)\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_size * 2,\n",
    "                            out_features = 1)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Sigmoide\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        \n",
    "        # HiddenSize\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        packed = pack_padded_sequence(embedded, text_len, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        packed_output, _ = self.lstm(packed)\n",
    "        \n",
    "        seq_unpacked, lens_unpacked = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        seq_unpacked_out_f = seq_unpacked[:, -1, :self.hidden_size]\n",
    "        \n",
    "        seq_unpacked_out_r = seq_unpacked[:, 0, self.hidden_size:]\n",
    "        \n",
    "        output = torch.cat((seq_unpacked_out_f, seq_unpacked_out_r), dim = 1)\n",
    "\n",
    "        predictions = self.fc(self.dropout(output))\n",
    "\n",
    "        return self.sigm(predictions).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pe2F0h8-KOHb"
   },
   "outputs": [],
   "source": [
    "modelo = Glove6B300BiLSTM(hidden_size= 180, text_field = documento_fd).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMfKHOKtgMdJ"
   },
   "source": [
    "2. Implemente el ciclo de entrenamiento de la red `Glove6B300BiLSTM`. Para ello use:\n",
    "- Entropía cruzada binaria como funcion de costo - `torch.nn.BCELoss()`\n",
    "- Adam cómo algoritmo de optimización, con `lr=2e-4`\n",
    "- 10 epocas de entrenamiento\n",
    "- Los `BucketIterator` definidos en la pregunta anterior para recorrer los conjuntos de entrenamiento y validación.\n",
    "- Guarde en una lista el historico de valores de la función de costo en el conjunto de entrenamiento y de validación\n",
    "- Al final de cada época guarde el modelo mediante `guardar_modelo` en la ruta `modelos/Glove6B300BiLSTM.h5`, si es que la función de costo sobre el conjunto de validación es menor que la menor función de costo sobre el conjunto de validación observada en épocas anteriores. \n",
    "\n",
    "Al finalizar el entrenamiento, muestre en pantalla un gráfico con el historico de la función de costo en el conjunto de entrenamiento y en el de validación y guardelo en `resultados/Glove6B300BiLSTM/costo_historico.pdf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GfD8U_YwpPeQ"
   },
   "outputs": [],
   "source": [
    "def save_model_all(model, save_dir, model_name):\n",
    "    \"\"\"\n",
    "    :param model:  nn model\n",
    "    :param save_dir: save model direction\n",
    "    :param model_name:  model name\n",
    "    :param epoch:  epoch\n",
    "    :return:  None\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_prefix = os.path.join(save_dir, model_name)\n",
    "    save_path = '{}.h5'.format(save_prefix)\n",
    "    print(\"save all model to {}\".format(save_path))\n",
    "    output = open(save_path, mode=\"wb\")\n",
    "    torch.save(model.state_dict(), output)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V3Z5YTlwwre3"
   },
   "outputs": [],
   "source": [
    "def register(res_list):\n",
    "    '''Obtiene la perdida promedio y su desviacion estandar en batches.'''\n",
    "    \n",
    "    losses, nums = zip(*res_list)\n",
    "    \n",
    "    N = np.sum(nums)\n",
    "    loss_mean = np.sum(np.multiply(losses, nums))/N\n",
    "    loss_std = np.sqrt(np.sum(np.multiply((losses-loss_mean)**2, nums))/(N-1))\n",
    "    \n",
    "    return loss_mean, loss_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MduL3nyFYmh5"
   },
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb[0], xb[1]), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzck7IpAYMxY"
   },
   "outputs": [],
   "source": [
    "def fit(epochs,\n",
    "        model,\n",
    "        loss_func,\n",
    "        opt,\n",
    "        train_dl,\n",
    "        valid_dl,\n",
    "        metric=None,\n",
    "        only_print=True,\n",
    "        print_leap = 1):\n",
    "\n",
    "    if metric is None:\n",
    "        metric = loss_func\n",
    "\n",
    "    learning_data = pd.DataFrame(\n",
    "        columns=['epoch', 'train_mean', 'train_std', 'val_mean', 'val_std'])\n",
    "    \n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Entrenamiento -------------------------------------------------------\n",
    "        train_res = []\n",
    "        model.train()\n",
    "\n",
    "        for x in train_dl:\n",
    "            # Para entrenar se usa la funcion de perdida\n",
    "            loss_batch(model, loss_func, x.pro_X, x.y, opt)\n",
    "\n",
    "            # Para almacenar se puede usar una metrica\n",
    "            train_res.append(loss_batch(model, metric, x.pro_X, x.y))\n",
    "\n",
    "        # Validacion ----------------------------------------------------------\n",
    "        # Para evaluar se puede utilizar un metrica de rendimiento\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_res = [\n",
    "                loss_batch(model, metric, x.pro_X, x.y) for x in valid_dl\n",
    "            ]\n",
    "\n",
    "        val_loss, val_std = register(val_res)\n",
    "        train_loss, train_std = register(train_res)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            save_model_all(model, 'modelos', 'Glove6B300BiLSTM', '')\n",
    "            best_loss = val_loss\n",
    "        \n",
    "        if epoch % print_leap == 0:\n",
    "            print('Epoca:', epoch, '- val:', val_loss, '- train:', train_loss)\n",
    "\n",
    "        learning_data = learning_data.append(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'train_mean': train_loss,\n",
    "                'train_std': train_std,\n",
    "                'val_mean': val_loss,\n",
    "                'val_std': val_std\n",
    "            },\n",
    "            ignore_index=True)\n",
    "\n",
    "    if only_print:\n",
    "        print('Proceso terminado')\n",
    "    else:\n",
    "        return learning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lx7Gx9eQORoE"
   },
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()\n",
    "opt = optim.Adam(modelo.parameters(), lr = 2e-4)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2BwVkAHsXqxx"
   },
   "outputs": [],
   "source": [
    "learning_data = fit(epochs,\n",
    "                    modelo,\n",
    "                    loss_func,\n",
    "                    opt,\n",
    "                    train_iter,\n",
    "                    val_iter,\n",
    "                    only_print=False,\n",
    "                    print_leap=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Nyee5O4yC92"
   },
   "outputs": [],
   "source": [
    "def show_learning_curves(learning_data, leaps = 1):\n",
    "    '''Genera curvas de aprendizaje dado data dataframe resultado de fit().'''\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=[10, 7])\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "    epcs = learning_data['epoch'][::leaps]\n",
    "    val_loss_lower = (learning_data['val_mean'] -\n",
    "                      learning_data['val_std'])[::leaps]\n",
    "\n",
    "    val_loss_upper = (learning_data['val_mean'] +\n",
    "                      learning_data['val_std'])[::leaps]\n",
    "\n",
    "    train_loss_lower = (learning_data['train_mean'] -\n",
    "                        learning_data['train_std'])[::leaps]\n",
    "    train_loss_upper = (learning_data['train_mean'] +\n",
    "                        learning_data['train_std'])[::leaps]\n",
    "\n",
    "    ax.plot(epcs,\n",
    "            learning_data['val_mean'][::leaps],\n",
    "            'o--',\n",
    "            color=\"g\",\n",
    "            label=\"Validation\")\n",
    "\n",
    "    ax.fill_between(epcs, val_loss_lower, val_loss_upper, alpha=0.1, color='g')\n",
    "\n",
    "    ax.plot(epcs,\n",
    "            learning_data['train_mean'][::leaps],\n",
    "            'o--',\n",
    "            color=\"r\",\n",
    "            label=\"Train\")\n",
    "\n",
    "    ax.fill_between(epcs, train_loss_lower, train_loss_upper, alpha=0.1, color='r')\n",
    "\n",
    "    ax.set_title('Curva de Aprendizaje', fontsize=25)\n",
    "    ax.set_xlabel('Epocas', fontsize=15)\n",
    "    ax.set_ylabel('Loss', fontsize=15)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6W4ooXwyHqH"
   },
   "outputs": [],
   "source": [
    "show_learning_curves(learning_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7m7lmgRqeUj"
   },
   "source": [
    "3. Obtenga la predicción de `modelo` sobre el conjunto de prueba y reporte el desempeño del clasificador mediante `evalua_sklearn` con la predicción de `modelo` sobre el conjunto de *prueba*, usando `nombre_clasificador='Glove6B300BiLSTM'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNp0XVDeqdq2"
   },
   "outputs": [],
   "source": [
    "modelo.load_state_dict(torch.load('/modelos/Glove6B300BiLSTMepoch.h5'))\n",
    "modelo.eval()\n",
    "trues = []\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        t, tl = batch.pro_X\n",
    "        trues.append(batch.y)\n",
    "\n",
    "        preds.append(modelo(t, tl).squeeze()) \n",
    "\n",
    "y_pred = torch.cat(preds).cpu()\n",
    "y_true = torch.cat(trues).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rk7cxFf2koTe"
   },
   "outputs": [],
   "source": [
    "evalua_sklearn(y_true, y_pred>0.5, 'Glove6B300BiLSTM')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "e3_procesamiento_de_texto_y_redes_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "es",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.367px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
